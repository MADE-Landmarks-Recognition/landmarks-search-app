{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import List, Tuple, Union, Optional\n",
    "\n",
    "import torch \n",
    "import yaml\n",
    "import faiss \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sparse\n",
    "import scipy.sparse.linalg as linalg\n",
    "from sklearn import preprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator, precision_at_k\n",
    "from omegaconf.dictconfig import DictConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from retrieval.engine.evaluate import evaluate, get_tester\n",
    "from retrieval.models.net import RetrievalNet\n",
    "from retrieval.getter import Getter\n",
    "\n",
    "data_dir = \"../../data/train10k/\"\n",
    "DEVICE = \"cuda\"\n",
    "WEIGHTS_PATH = Path(\"../experiments/ROADMAP/GLDv2_ROADMAP_128/weights/rolling.ckpt\") # Path(\"../experiments/ROADMAP/GLDv2_ROADMAP_classification_splits_resnet_440/weights/rolling.ckpt\") #\n",
    "getter = Getter()\n",
    "ID2CLS = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cfg(cfg_path):\n",
    "    with open(cfg_path, \"r\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    return DictConfig(cfg)\n",
    "\n",
    "\n",
    "def recall_at_k(targets, predictions, k):\n",
    "    s = sum(t in torch.Tensor(y).long()[:k] for t, y in zip(targets, predictions))\n",
    "    return s / len(targets)\n",
    "\n",
    "def global_average_precision_at_k(\n",
    "    knn_labels, query_labels, knn_distances, num_k=1\n",
    "):\n",
    "    knn_labels = knn_labels[:, :num_k]\n",
    "    knn_distances = knn_distances[:, :num_k]\n",
    "    num_samples = knn_labels.shape[0]\n",
    "    query_labels, knn_labels, knn_distances = (\n",
    "        query_labels.flatten(),\n",
    "        knn_labels.flatten(),\n",
    "        knn_distances.flatten(),\n",
    "    )\n",
    "    _, indices = knn_distances.sort(dim=0, descending=True)\n",
    "    sorted_query_labels = query_labels[indices]\n",
    "    sorted_knn_labels = knn_labels[indices]\n",
    "    relevance = torch.eq(sorted_query_labels, sorted_knn_labels)\n",
    "    cumulative_correct = torch.cumsum(relevance, dim=0)\n",
    "    precision = cumulative_correct / torch.arange(1, num_samples + 1)\n",
    "    gap = (precision * relevance).sum() / num_samples\n",
    "    return gap.float().item()\n",
    "\n",
    "\n",
    "def embedding_inference(data_loader, model):\n",
    "    model.eval()\n",
    "\n",
    "    all_predicts, all_targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(data_loader)):\n",
    "            input_, target = data[\"image\"], data[\"label\"]\n",
    "            input_ = input_.to(DEVICE)\n",
    "            emb = model(input_)\n",
    "            all_predicts.append(emb.squeeze().cpu())\n",
    "            all_targets.append(target)\n",
    "\n",
    "    predicts = torch.cat(all_predicts)\n",
    "    targets = torch.cat(all_targets)\n",
    "\n",
    "    return predicts, torch.tensor([ID2CLS[id_.item()] for id_ in targets.flatten()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalNet(\n",
       "  (backbone): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (pooling): Identity()\n",
       "  (standardize): Identity()\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = RetrievalNet(\n",
    "    \"vit_deit_distilled\",\n",
    "    embed_dim=384,\n",
    "    norm_features=False,\n",
    "    without_fc=True,\n",
    "    with_autocast=True,\n",
    ")\n",
    "# net = RetrievalNet(\n",
    "#     \"resnet18\",\n",
    "#     embed_dim=512,\n",
    "#     norm_features=False,\n",
    "#     without_fc=True,\n",
    "#     with_autocast=True,\n",
    "# )\n",
    "\n",
    "weights = torch.load(WEIGHTS_PATH, map_location=DEVICE)[\"net_state\"]\n",
    "net.load_state_dict(weights)\n",
    "net.to(DEVICE)\n",
    "net.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78192 39482\n"
     ]
    }
   ],
   "source": [
    "transform_cfg = load_cfg(\"../retrieval/config/transform/gldv2.yaml\")\n",
    "dataset_cfg = load_cfg(\"../retrieval/config/dataset/gldv2_10k_classification_splits.yaml\")\n",
    "test_transform = getter.get_transform(transform_cfg.test)\n",
    "test_ds = getter.get_dataset(test_transform, 'test', dataset_cfg)\n",
    "train_ds = getter.get_dataset(test_transform, 'train', dataset_cfg)\n",
    "print(len(train_ds), len(test_ds))\n",
    "CLASSES = Path(\"./classes.txt\").read_text().split() if Path(\"./classes.txt\").exists() else train_ds.classes_\n",
    "ID2CLS = {\n",
    "    i: int(cls)\n",
    "    for i, cls in enumerate(CLASSES)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN & ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseKNN(object):\n",
    "    \"\"\"KNN base class\"\"\"\n",
    "    def __init__(self, embeddings: np.ndarray, ids: Optional[np.ndarray], method):\n",
    "        if embeddings.dtype != np.float32:\n",
    "            embeddings = embeddings.astype(np.float32)\n",
    "        self.N = len(embeddings)\n",
    "        self.D = embeddings[0].shape[-1]\n",
    "        self.embeddings = embeddings if embeddings.flags['C_CONTIGUOUS'] \\\n",
    "                               else np.ascontiguousarray(embeddings)\n",
    "        self.labels = ids\n",
    "\n",
    "    def add(self, batch_size=10000):\n",
    "        \"\"\"Add data into index\"\"\"\n",
    "        if self.N <= batch_size:\n",
    "            self.index.add(self.embeddings)\n",
    "        else:\n",
    "            [self.index.add(self.embeddings[i:i+batch_size])\n",
    "                    for i in range(0, len(self.embeddings), batch_size)]\n",
    "\n",
    "    def search(self, queries, k=5):\n",
    "        \"\"\"Search\n",
    "        Args:\n",
    "            queries: query vectors\n",
    "            k: get top-k results\n",
    "        Returns:\n",
    "            sims: similarities of k-NN\n",
    "            ids: indexes of k-NN\n",
    "        \"\"\"\n",
    "        if not queries.flags['C_CONTIGUOUS']:\n",
    "            queries = np.ascontiguousarray(queries)\n",
    "        if queries.dtype != np.float32:\n",
    "            queries = queries.astype(np.float32)\n",
    "        sims, ids = self.index.search(queries, k)\n",
    "        \n",
    "        ids = self.labels[ids] if self.labels else ids\n",
    "        return sims, ids\n",
    "\n",
    "\n",
    "class KNN(BaseKNN):\n",
    "    \"\"\"KNN class\n",
    "    Args:\n",
    "        embeddings: feature vectors in database\n",
    "        ids: labels of feature vectors\n",
    "        method: distance metric\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings: np.ndarray, ids: np.ndarray, method):\n",
    "        super().__init__(embeddings, ids, method)\n",
    "        self.index = {\n",
    "            'cosine': faiss.IndexFlatIP,\n",
    "            'euclidean': faiss.IndexFlatL2,\n",
    "        }[method](self.D)\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES'):\n",
    "            self.index = faiss.index_cpu_to_all_gpus(self.index)\n",
    "        self.labels = ids\n",
    "        self.add()\n",
    "\n",
    "\n",
    "class ANN(BaseKNN):\n",
    "    \"\"\"Approximate nearest neighbor search class\n",
    "    Args:\n",
    "        embeddings: feature vectors in database\n",
    "        ids: labels of feature vectors\n",
    "        method: distance metric\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, embeddings: np.ndarray, ids: Optional[np.ndarray], method=\"cosine\", M=128, nbits=8, nlist=316, nprobe=64\n",
    "    ):\n",
    "        super().__init__(embeddings, ids, method)\n",
    "        self.labels = ids\n",
    "        self.quantizer = {\n",
    "            'cosine': faiss.IndexFlatIP,\n",
    "            'euclidean': faiss.IndexFlatL2\n",
    "        }[method](self.D)\n",
    "        self.index = faiss.IndexIVFPQ(self.quantizer, self.D, nlist, M, nbits)\n",
    "        samples = embeddings[np.random.permutation(np.arange(self.N))[:self.N // 5]]\n",
    "        self.index.train(samples)\n",
    "        self.add()\n",
    "        self.index.nprobe = nprobe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_path = Path(\"./cache_embeddings/train_emb.npz\")\n",
    "train_targets_path = Path(\"./cache_embeddings/train_targets.npz\")\n",
    "test_embeddings_path = Path(\"./cache_embeddings/test_emb.npz\")\n",
    "test_targets_path = Path(\"./cache_embeddings/test_targets.npz\")\n",
    "\n",
    "if not train_embeddings_path.exists():\n",
    "    train_embeddings, train_targets = embedding_inference(train_loader, net)\n",
    "    train_embeddings, train_targets = train_embeddings.numpy(), train_targets.numpy()\n",
    "    np.savez_compressed(train_embeddings_path, embeddigs=train_embeddings)\n",
    "    np.savez_compressed(train_targets_path, embeddigs=train_targets)\n",
    "else:\n",
    "    train_embeddings = np.load(train_embeddings_path)[\"embeddigs\"]\n",
    "    train_targets = np.load(train_targets_path)[\"embeddigs\"]\n",
    "\n",
    "if not test_embeddings_path.exists():\n",
    "    queries, targets = embedding_inference(test_loader, net)\n",
    "    queries, targets = queries.numpy(), targets.numpy()\n",
    "    np.savez_compressed(test_embeddings_path, embeddigs=queries)\n",
    "    np.savez_compressed(test_targets_path, embeddigs=targets)\n",
    "else:\n",
    "    queries = np.load(test_embeddings_path)[\"embeddigs\"]\n",
    "    targets = np.load(test_targets_path)[\"embeddigs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[index] add: 100%|██████████| 16/16 [00:00<00:00, 17.43it/s]\n",
      "[index] add: 100%|██████████| 16/16 [00:31<00:00,  1.96s/it]\n"
     ]
    }
   ],
   "source": [
    "db_cosine = KNN(train_embeddings, train_targets, method=\"cosine\")\n",
    "# db_euclidean = KNN(train_embeddings.numpy(), train_targets.numpy(), method=\"euclidean\")\n",
    "# db_ann = ANN(train_embeddings.numpy(), train_targets.numpy(), method=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANN cosine\n",
      "R@1 = 0.8066\n",
      "R@5 = 0.8851\n",
      "GAP@1 = 0.5190\n",
      "\n",
      "Cosine\n",
      "R@1 = 0.8112\n",
      "R@5 = 0.8887\n",
      "GAP@1 = 0.7799\n"
     ]
    }
   ],
   "source": [
    "dbs = [db_cosine]#, db_euclidean, db_ann]\n",
    "names = [\"Cosine\"]#, \"Euclidean\", \"ANN cosine\"]\n",
    "for db, name in zip(dbs, names):\n",
    "    t = time.time()\n",
    "    sims, ids = db.search(queries.numpy(), k=5)\n",
    "    search_time = time.time() - t\n",
    "    print(f\"\\n{name} - {search_time:.4f} secs\")\n",
    "    print(f\"R@1 = {recall_at_k(targets, ids, k=1):.4f}\")\n",
    "    print(f\"R@5 = {recall_at_k(targets, ids, k=5):.4f}\")\n",
    "    print(f\"GAP@1 = {global_average_precision_at_k(torch.from_numpy(ids), targets, torch.from_numpy(sims)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ResNet18\n",
    "R@1 = 0.8112\n",
    "R@5 = 0.8887\n",
    "GAP@1 = 0.7799\n",
    "\n",
    "DeiT\n",
    "R@1 = 0.9032\n",
    "R@5 = 0.9507\n",
    "GAP = 0.8940\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_ids = None\n",
    "trunc_init = None\n",
    "lap_alpha = None\n",
    "\n",
    "\n",
    "def get_offline_result(i):\n",
    "    ids = trunc_ids[i]\n",
    "    trunc_lap = lap_alpha[ids][:, ids]\n",
    "    scores, _ = linalg.cg(trunc_lap, trunc_init, tol=1e-6, maxiter=20)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def cache(filename):\n",
    "    \"\"\"Decorator to cache results\"\"\"\n",
    "\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kw):\n",
    "            self = args[0]\n",
    "            path = os.path.join(self.cache_dir, filename)\n",
    "            Path(self.cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "            time0 = time.time()\n",
    "            if os.path.exists(path):\n",
    "                result = joblib.load(path)\n",
    "                cost = time.time() - time0\n",
    "                # print(\"[cache] loading {} costs {:.2f}s\".format(path, cost))\n",
    "                return result\n",
    "            result = func(*args, **kw)\n",
    "            cost = time.time() - time0\n",
    "            print(\"[cache] obtaining {} costs {:.2f}s\".format(path, cost))\n",
    "            joblib.dump(result, path)\n",
    "            return result\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "class Diffusion(object):\n",
    "    \"\"\"Diffusion class\"\"\"\n",
    "\n",
    "    def __init__(self, features, labels, cache_dir):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.N = len(self.features)\n",
    "        self.cache_dir = cache_dir\n",
    "        # use ANN for large datasets\n",
    "        self.use_ann = self.N >= 1_000_000\n",
    "        if self.use_ann:\n",
    "            print(\"ANN creating...\")\n",
    "            self.ann = ANN(self.features, self.labels, method=\"cosine\")\n",
    "        self.knn = KNN(self.features, self.labels, method=\"cosine\")\n",
    "\n",
    "    @cache(\"offline.jbl\")\n",
    "    def get_offline_results(self, n_trunc, kd=50):\n",
    "        \"\"\"Get offline diffusion results for each gallery feature\"\"\"\n",
    "        print(\"[offline] starting offline diffusion\")\n",
    "        print(\"[offline] 1) prepare Laplacian and initial state\")\n",
    "        global trunc_ids, trunc_init, lap_alpha\n",
    "        if self.use_ann:\n",
    "            _, trunc_ids = self.ann.search(self.features, n_trunc)\n",
    "            sims, ids = self.knn.search(self.features, kd)\n",
    "            lap_alpha = self.get_laplacian(sims, ids)\n",
    "        else:\n",
    "            sims, ids = self.knn.search(self.features, n_trunc)\n",
    "            trunc_ids = ids\n",
    "            lap_alpha = self.get_laplacian(sims[:, :kd], ids[:, :kd])\n",
    "        trunc_init = np.zeros(n_trunc)\n",
    "        trunc_init[0] = 1\n",
    "\n",
    "        print(\"[offline] 2) gallery-side diffusion\")\n",
    "        results = Parallel(n_jobs=-1, prefer=\"threads\")(\n",
    "            delayed(get_offline_result)(i)\n",
    "            for i in tqdm(range(self.N), desc=\"[offline] diffusion\")\n",
    "        )\n",
    "        all_scores = np.concatenate(results)\n",
    "\n",
    "        print(\"[offline] 3) merge offline results\")\n",
    "        rows = np.repeat(np.arange(self.N), n_trunc)\n",
    "\n",
    "        offline = sparse.csr_matrix(\n",
    "            (all_scores, (rows, trunc_ids.reshape(-1))),\n",
    "            shape=(self.N, self.N),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        return offline\n",
    "\n",
    "    # @cache('laplacian.jbl')\n",
    "    def get_laplacian(self, sims, ids, alpha=0.99):\n",
    "        \"\"\"Get Laplacian_alpha matrix\"\"\"\n",
    "        affinity = self.get_affinity(sims, ids)\n",
    "        num = affinity.shape[0]\n",
    "        degrees = affinity @ np.ones(num) + 1e-12\n",
    "        # mat: degree matrix ^ (-1/2)\n",
    "        mat = sparse.dia_matrix(\n",
    "            (degrees ** (-0.5), [0]), shape=(num, num), dtype=np.float32\n",
    "        )\n",
    "        stochastic = mat @ affinity @ mat\n",
    "        sparse_eye = sparse.dia_matrix(\n",
    "            (np.ones(num), [0]), shape=(num, num), dtype=np.float32\n",
    "        )\n",
    "        lap_alpha = sparse_eye - alpha * stochastic\n",
    "        return lap_alpha\n",
    "\n",
    "    # @cache('affinity.jbl')\n",
    "    def get_affinity(self, sims, ids, gamma=3):\n",
    "        \"\"\"Create affinity matrix for the mutual kNN graph of the whole dataset\n",
    "        Args:\n",
    "            sims: similarities of kNN\n",
    "            ids: indexes of kNN\n",
    "        Returns:\n",
    "            affinity: affinity matrix\n",
    "        \"\"\"\n",
    "        num = sims.shape[0]\n",
    "        sims[sims < 0] = 0  # similarity should be non-negative\n",
    "        sims = sims**gamma\n",
    "        # vec_ids: feature vectors' ids\n",
    "        # mut_ids: mutual (reciprocal) nearest neighbors' ids\n",
    "        # mut_sims: similarites between feature vectors and their mutual nearest neighbors\n",
    "        vec_ids, mut_ids, mut_sims = [], [], []\n",
    "        for i in range(num):\n",
    "            # check reciprocity: i is in j's kNN and j is in i's kNN when i != j\n",
    "            ismutual = np.isin(ids[ids[i]], i).any(axis=1)\n",
    "            ismutual[0] = False\n",
    "            if ismutual.any():\n",
    "                vec_ids.append(i * np.ones(ismutual.sum(), dtype=int))\n",
    "                mut_ids.append(ids[i, ismutual])\n",
    "                mut_sims.append(sims[i, ismutual])\n",
    "        vec_ids, mut_ids, mut_sims = map(np.concatenate, [vec_ids, mut_ids, mut_sims])\n",
    "        affinity = sparse.csc_matrix(\n",
    "            (mut_sims, (vec_ids, mut_ids)), shape=(num, num), dtype=np.float32\n",
    "        )\n",
    "        return affinity\n",
    "\n",
    "\n",
    "def offline_diffusion_search(\n",
    "    queries,\n",
    "    diffusion,\n",
    "    truncation_size=1000,\n",
    "    kd=5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        queries: predicted embeddings\n",
    "        gallery: train embeddings\n",
    "        cache_dir: Directory to cache embeddings\n",
    "        truncation_size: Number of images in the truncated gallery\n",
    "        kd: top k results\n",
    "    \"\"\"\n",
    "    # t = time.time()\n",
    "    n_query = len(queries)\n",
    "    offline = diffusion.get_offline_results(truncation_size, kd)\n",
    "    # nan_idx = [i for i, row in enumerate(tqdm(offline.data)) if not np.any(np.isnan(row))]\n",
    "    # print(len(nan_idx))\n",
    "    # print(offline.shape)\n",
    "    # offline.data = offline.data[nan_idx, :]\n",
    "\n",
    "    # print(\"Search NaNs\")\n",
    "    # cnt = 0\n",
    "    # nan_indices = []\n",
    "    # for i in tqdm(range(offline.shape[0])):\n",
    "    #     a = offline[i].toarray()\n",
    "    #     is_nans = np.any(np.isnan(a))\n",
    "    #     if is_nans:\n",
    "    #         cnt += 1\n",
    "    #         nan_indices.append(i)\n",
    "    # print(f\"find {cnt} embeddings with NaNs\")\n",
    "    # print(f\"NaN indices: {nan_indices}\")\n",
    "\n",
    "    # print(\"fill nans\")\n",
    "    # offline.data[np.isnan(offline.data)] = 0.0\n",
    "    # print(\"normalize\")\n",
    "    features = preprocessing.normalize(offline, norm=\"l2\", axis=1)\n",
    "    # print(\"count scores\")\n",
    "    scores = features[:n_query] @ features[n_query:].T\n",
    "    # print(\"sort\")\n",
    "    ranks = np.argsort(-scores.toarray())\n",
    "    # ranks = np.argpartition(-scores.todense(), -kd)[-kd:]\n",
    "    # ranks = np.argmax(scores.todense(), axis=1)\n",
    "    # all_pipeline_time = time.time() - t\n",
    "    # print(f\"All Pipeline Time: {all_pipeline_time:.4f} seconds\")\n",
    "    return ranks[:, :kd]\n",
    "\n",
    "\n",
    "# off_ids = offline_diffusion_search(\n",
    "#     queries[:100], train_embeddings, None, truncation_size=10, kd=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diffusion prediction:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "[index] add:  38%|███▊      | 3/8 [04:10<06:57, 83.44s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [54], line 14\u001b[0m\n\u001b[1;32m      8\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOffline Diffusion\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m     diffusion \u001b[39m=\u001b[39m Diffusion(\n\u001b[1;32m     10\u001b[0m         np\u001b[39m.\u001b[39mvstack([q, train_embeddings]),\n\u001b[1;32m     11\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m         cache_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/home/and/projects/made/landmarks-recognition/andrey-research-repo/notebooks/cache_embeddings\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     )\n\u001b[0;32m---> 14\u001b[0m     ids \u001b[39m=\u001b[39m offline_diffusion_search(q, diffusion, truncation_size\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, kd\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[1;32m     15\u001b[0m     predicts\u001b[39m.\u001b[39mappend(train_targets[ids])\n\u001b[1;32m     16\u001b[0m predicts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(predicts)\n",
      "Cell \u001b[0;32mIn [47], line 172\u001b[0m, in \u001b[0;36moffline_diffusion_search\u001b[0;34m(queries, diffusion, truncation_size, kd)\u001b[0m\n\u001b[1;32m    151\u001b[0m offline \u001b[39m=\u001b[39m diffusion\u001b[39m.\u001b[39mget_offline_results(truncation_size, kd)\n\u001b[1;32m    152\u001b[0m \u001b[39m# nan_idx = [i for i, row in enumerate(tqdm(offline.data)) if not np.any(np.isnan(row))]\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[39m# print(len(nan_idx))\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39m# print(offline.shape)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39m# offline.data[np.isnan(offline.data)] = 0.0\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# print(\"normalize\")\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m features \u001b[39m=\u001b[39m preprocessing\u001b[39m.\u001b[39;49mnormalize(offline, norm\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39ml2\u001b[39;49m\u001b[39m\"\u001b[39;49m, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    173\u001b[0m \u001b[39m# print(\"count scores\")\u001b[39;00m\n\u001b[1;32m    174\u001b[0m scores \u001b[39m=\u001b[39m features[:n_query] \u001b[39m@\u001b[39m features[n_query:]\u001b[39m.\u001b[39mT\n",
      "File \u001b[0;32m~/miniforge3/envs/landmarks/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:1786\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1784\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is not a supported axis\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m axis)\n\u001b[0;32m-> 1786\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1787\u001b[0m     X,\n\u001b[1;32m   1788\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49msparse_format,\n\u001b[1;32m   1789\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1790\u001b[0m     estimator\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthe normalize function\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1791\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[1;32m   1792\u001b[0m )\n\u001b[1;32m   1793\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1794\u001b[0m     X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mT\n",
      "File \u001b[0;32m~/miniforge3/envs/landmarks/lib/python3.9/site-packages/sklearn/utils/validation.py:822\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[39mif\u001b[39;00m sp\u001b[39m.\u001b[39missparse(array):\n\u001b[1;32m    821\u001b[0m     _ensure_no_complex_data(array)\n\u001b[0;32m--> 822\u001b[0m     array \u001b[39m=\u001b[39m _ensure_sparse_format(\n\u001b[1;32m    823\u001b[0m         array,\n\u001b[1;32m    824\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m    825\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    826\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    827\u001b[0m         force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m    828\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m    829\u001b[0m         estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    830\u001b[0m         input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    831\u001b[0m     )\n\u001b[1;32m    832\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[39m# If np.array(..) gives ComplexWarning, then we convert the warning\u001b[39;00m\n\u001b[1;32m    834\u001b[0m     \u001b[39m# to an error. This is needed because specifying a non complex\u001b[39;00m\n\u001b[1;32m    835\u001b[0m     \u001b[39m# dtype to the function converts complex to real dtype,\u001b[39;00m\n\u001b[1;32m    836\u001b[0m     \u001b[39m# thereby passing the test made in the lines following the scope\u001b[39;00m\n\u001b[1;32m    837\u001b[0m     \u001b[39m# of warnings context manager.\u001b[39;00m\n\u001b[1;32m    838\u001b[0m     \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n",
      "File \u001b[0;32m~/miniforge3/envs/landmarks/lib/python3.9/site-packages/sklearn/utils/validation.py:551\u001b[0m, in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    546\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    547\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt check \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m sparse matrix for nan or inf.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m spmatrix\u001b[39m.\u001b[39mformat,\n\u001b[1;32m    548\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m    549\u001b[0m         )\n\u001b[1;32m    550\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 551\u001b[0m         _assert_all_finite(\n\u001b[1;32m    552\u001b[0m             spmatrix\u001b[39m.\u001b[39;49mdata,\n\u001b[1;32m    553\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    554\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    555\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    556\u001b[0m         )\n\u001b[1;32m    558\u001b[0m \u001b[39mreturn\u001b[39;00m spmatrix\n",
      "File \u001b[0;32m~/miniforge3/envs/landmarks/lib/python3.9/site-packages/sklearn/utils/validation.py:146\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    125\u001b[0m             \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[1;32m    126\u001b[0m             \u001b[39mand\u001b[39;00m estimator_name\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    131\u001b[0m             \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m             msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    133\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m             )\n\u001b[0;32m--> 146\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n\u001b[1;32m    148\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "batch_size = 4096\n",
    "recalls = []\n",
    "predicts = []\n",
    "\n",
    "t = time.time()\n",
    "for i in tqdm(range(0, len(queries), batch_size), desc=\"Diffusion prediction\"):\n",
    "    q = queries[i : i + batch_size]\n",
    "    name = \"Offline Diffusion\"\n",
    "    diffusion = Diffusion(\n",
    "        np.vstack([q, train_embeddings]),\n",
    "        None,\n",
    "        cache_dir=\"/home/and/projects/made/landmarks-recognition/andrey-research-repo/notebooks/cache_embeddings\",\n",
    "    )\n",
    "    ids = offline_diffusion_search(q, diffusion, truncation_size=10, kd=5)\n",
    "    predicts.append(train_targets[ids])\n",
    "predicts = np.concatenate(predicts)\n",
    "search_time = time.time() - t\n",
    "\n",
    "sims = np.repeat([i / 10 for i in range(9, 4)], len(queries))\n",
    "print(f\"\\n{name} - {search_time:.4f} secs\")\n",
    "print(f\"R@1 = {recall_at_k(targets[:batch_size], predicts, k=1):.4f}\")\n",
    "# print(f\"R@5 = {recall_at_k(targets, predicts, k=5):.4f}\")\n",
    "# print(f\"GAP@1 = {global_average_precision_at_k(torch.from_numpy(ids),  torch.from_numpy(targets), torch.from_numpy(sims)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 44244, 174711,  64263,  77126,  26841,  48168, 176018,  13111,\n",
       "        62440, 191756,  79505, 202154, 172056, 105193,   6669, 128357,\n",
       "       202886, 107718, 126255,  55235])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[177599, 177599, 177599, 194198, 194198],\n",
       "       [ 42848,  42848,  42848,  42848,  42848],\n",
       "       [  7112,  74090,  74090,  74090, 137927],\n",
       "       ...,\n",
       "       [  7112, 121157, 121157, 121157,  21334],\n",
       "       [  7112, 121157, 121157, 121157,  21334],\n",
       "       [  7112, 121157, 121157, 121157,  21334]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('landmarks-app-8FyqVnJp-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6837259c64425f9974ee2af9fd36cc42850b53c582df4408ed0f3b156f9f0bc1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
